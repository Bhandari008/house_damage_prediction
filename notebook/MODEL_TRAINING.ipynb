{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c9d417c",
   "metadata": {},
   "source": [
    "# Model Training\n",
    "\n",
    "## 1.1 Import Data and Required Packages\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f23bb881",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from imblearn.over_sampling import SMOTE, RandomOverSampler\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f1b876db",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_features = pd.read_csv(\"data/train_values.csv\")\n",
    "df_labels = pd.read_csv(\"data/train_labels.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "68f82113",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.merge(df_features, df_labels, on='building_id')\n",
    "df = df.drop(columns='building_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76012bf7",
   "metadata": {},
   "source": [
    "### Preparing X and y variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "93f16a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns = [\"damage_grade\"])\n",
    "y = df[\"damage_grade\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "000bfa89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X=(260601, 38), Shape of y=(260601,)\n"
     ]
    }
   ],
   "source": [
    "### Sanity Check\n",
    "print(f\"Shape of X={(X.shape)}, Shape of y={(y.shape)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b8a1b7",
   "metadata": {},
   "source": [
    "### Convert categorical data to numerical data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f8e217a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = ['land_surface_condition', 'foundation_type', 'roof_type','ground_floor_type', \n",
    "                          'other_floor_type', 'position', 'plan_configuration', 'legal_ownership_status']\n",
    "# label encoding categorical columns in train dataset \n",
    "X[cat_features] = X[cat_features].apply(lambda x: x.astype('category').cat.codes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a96a09a",
   "metadata": {},
   "source": [
    "### Scaling the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27ba0d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b2495f",
   "metadata": {},
   "source": [
    "### Splitting into train, validation and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95a187c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53a145cc",
   "metadata": {},
   "source": [
    "* Number of data points in Train set : 70 % of total no. of data points\n",
    "* Number of data points in Validation set : 10 % of total no. of data points\n",
    "* Number of data points in Test set : 20 % of total no. of data points\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6b012d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c32cebe7",
   "metadata": {},
   "source": [
    "### Create an Evaluation function to give all metrics after model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "43a3e5e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(true, predicted):\n",
    "    \"\"\"\n",
    "    Generate the confusion matrix and classification report\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(true, predicted)\n",
    "    cr = classification_report(true, predicted)\n",
    "    conf_matrix=pd.DataFrame(data=cm,columns=['Predicted:1','Predicted:2','Predicted:3'],\n",
    "                                         index=['Actual:1','Actual:2','Actual:3'])\n",
    "    # confusion matrix in heatmap\n",
    "    sns.heatmap(conf_matrix, annot=True, cmap='Blues', fmt='g')\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "    print(cr) \n",
    "    f1_micro = f1_score(true, predicted, average='micro')\n",
    "    return f1_micro"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfae83f2",
   "metadata": {},
   "source": [
    "## Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a87cb984",
   "metadata": {},
   "source": [
    "### Without over and under sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d56522e",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
    "    \"AdaBoost Classifier\": AdaBoostClassifier(),\n",
    "    \"DecisionTree Classifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "model_list = []\n",
    "f1_micro = []\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate Train and Validation dataset\n",
    "    model_train_f1 = evaluate_model(y_train, y_train_pred)\n",
    "    model_test_f1 = evaluate_model(y_test, y_test_pred)\n",
    "    \n",
    "    print(list(models.keys())[i])\n",
    "    model_list.append(list(models.keys())[i])\n",
    "    \n",
    "    print(\"Model Performance for Training Set\")\n",
    "    print(\"F1 Micro-Averaged Score: {:.4f}\".format(model_train_f1))\n",
    "    print(\"--------------------------------------\")\n",
    "    \n",
    "    print(\"Model Performance for Validation Set\")\n",
    "    print(\"F1 Micro-Averaged Score: {:.4f}\".format(model_test_f1))\n",
    "    \n",
    "    f1_micro.append(model_test_f1)\n",
    "    \n",
    "    print('='*35)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5320f455",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e446e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_without_sampling=pd.DataFrame(list(zip(model_list, f1_micro)), columns=['Model Name', 'Micro Averaged F1-Score']).sort_values(by=[\"Micro Averaged F1-Score\"],ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7817dd",
   "metadata": {},
   "source": [
    "### ii. Oversampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897844a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_resample, y_train_resample = SMOTE().fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c5afdaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    \"Random Forest\": RandomForestClassifier(),\n",
    "    \"Gradient Boosting Classifier\": GradientBoostingClassifier(),\n",
    "    \"AdaBoost Classifier\": AdaBoostClassifier(),\n",
    "    \"DecisionTree Classifier\": DecisionTreeClassifier()\n",
    "}\n",
    "\n",
    "model_list = []\n",
    "f1_micro = []\n",
    "\n",
    "for i in range(len(list(models))):\n",
    "    model = list(models.values())[i]\n",
    "    model.fit(X_train_resample, y_train_resample)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = model.predict(X_train_resample)\n",
    "    y_test_pred = model.predict(X_test)\n",
    "    \n",
    "    # Evaluate Train and Validation dataset\n",
    "    model_train_f1 = evaluate_model(y_test_resample, y_train_pred)\n",
    "    model_test_f1 = evaluate_model(y_test, y_val_pred)\n",
    "    \n",
    "    print(list(models.keys())[i])\n",
    "    model_list.append(list(models.keys())[i])\n",
    "    \n",
    "    print(\"Model Performance for Training Set\")\n",
    "    print(\"F1 Micro-Averaged Score: {:.4f}\".format(model_train_f1))\n",
    "    print(\"--------------------------------------\")\n",
    "    \n",
    "    print(\"Model Performance for Validation Set\")\n",
    "    print(\"F1 Micro-Averaged Score: {:.4f}\".format(model_test_f1))\n",
    "    \n",
    "    f1_micro.append(model_test_f1)\n",
    "    \n",
    "    print('='*35)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff7b5f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_smote = pd.DataFrame(list(zip(model_list, f1_micro)), columns=['Model Name', 'Micro Averaged F1-Score']).sort_values(by=[\"Micro Averaged F1-Score\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f354260",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_with_smote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64b79ae",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
